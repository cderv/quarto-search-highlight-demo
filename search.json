[
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "A well-designed chart communicates patterns in data faster than any table or summary statistic. Visualization is not just a presentation tool — it is an analytical tool. Plotting your data early and often reveals outliers, trends, and relationships that would otherwise remain hidden in rows of numbers.\nThe choice of chart type depends on what aspect of the data you want to emphasize. Bar charts compare categories, line charts show trends over time, scatter plots reveal relationships between variables, and histograms display distributions."
  },
  {
    "objectID": "visualization.html#why-visualization-matters",
    "href": "visualization.html#why-visualization-matters",
    "title": "Data Visualization",
    "section": "",
    "text": "A well-designed chart communicates patterns in data faster than any table or summary statistic. Visualization is not just a presentation tool — it is an analytical tool. Plotting your data early and often reveals outliers, trends, and relationships that would otherwise remain hidden in rows of numbers.\nThe choice of chart type depends on what aspect of the data you want to emphasize. Bar charts compare categories, line charts show trends over time, scatter plots reveal relationships between variables, and histograms display distributions."
  },
  {
    "objectID": "visualization.html#creating-basic-plots",
    "href": "visualization.html#creating-basic-plots",
    "title": "Data Visualization",
    "section": "Creating Basic Plots",
    "text": "Creating Basic Plots\nMost Python visualization starts with matplotlib, the foundational plotting library. While its syntax can be verbose, matplotlib provides fine-grained control over every aspect of a chart. The pyplot interface offers a quick way to create common plot types.\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(dates, values, marker=\"o\")\nplt.title(\"Monthly Revenue\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Revenue ($)\")\nplt.grid(True)\nplt.show()\nFor statistical visualization, seaborn builds on matplotlib with a higher-level interface. It produces attractive charts with minimal code and integrates directly with pandas DataFrames."
  },
  {
    "objectID": "visualization.html#chart-types-and-when-to-use-them",
    "href": "visualization.html#chart-types-and-when-to-use-them",
    "title": "Data Visualization",
    "section": "Chart Types and When to Use Them",
    "text": "Chart Types and When to Use Them\nBar charts work best for comparing discrete categories. Use horizontal bar charts when category labels are long. Grouped or stacked bars can show subcategories within each group.\nLine charts are ideal for time series data where you want to show trends and changes over a continuous axis. Multiple lines on the same plot allow direct comparison between series.\nScatter plots reveal the relationship between two numeric variables. Adding color or size as a third dimension can encode additional data. A scatter plot is often the first step before fitting a regression model to your data.\nBox plots summarize the distribution of a numeric variable by showing the median, quartiles, and outliers. They are especially useful for comparing distributions across groups."
  },
  {
    "objectID": "visualization.html#interactive-visualization",
    "href": "visualization.html#interactive-visualization",
    "title": "Data Visualization",
    "section": "Interactive Visualization",
    "text": "Interactive Visualization\nStatic charts work well for reports and publications, but interactive visualization lets users explore data at their own pace. Libraries like Plotly and Altair produce interactive charts that support zooming, panning, tooltips, and filtering.\nimport plotly.express as px\n\nfig = px.scatter(\n    df, x=\"gdp_per_capita\", y=\"life_expectancy\",\n    size=\"population\", color=\"continent\",\n    hover_name=\"country\",\n    title=\"Health vs Wealth by Country\"\n)\nfig.show()\nInteractive charts are particularly effective in dashboards where users need to drill into specific data points. The ability to hover over a chart element and see its exact values eliminates the need to cross-reference between the chart and the underlying data table."
  },
  {
    "objectID": "visualization.html#design-principles",
    "href": "visualization.html#design-principles",
    "title": "Data Visualization",
    "section": "Design Principles",
    "text": "Design Principles\nGood chart design follows a few core principles. Minimize non-data ink — every element on the chart should serve a purpose. Label axes clearly and include units. Use color intentionally, not decoratively. Avoid 3D effects that distort data perception.\nThe data-to-ink ratio, a concept from Edward Tufte, suggests removing any visual element that does not directly represent data. Grid lines should be subtle, legends should be positioned close to the data they describe, and chart titles should state the finding, not just the topic."
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "Statistical Modeling",
    "section": "",
    "text": "Statistical modeling is the process of building a mathematical representation of the relationships in your data. A model captures the patterns and structure in observed data, allowing you to make predictions, test hypotheses, and understand which variables drive outcomes.\nThe modeling workflow starts with exploratory data analysis — understanding your data before fitting any model. Visualizing distributions, checking for correlations, and identifying potential confounders are essential steps before writing a single line of modeling code."
  },
  {
    "objectID": "modeling.html#from-data-to-models",
    "href": "modeling.html#from-data-to-models",
    "title": "Statistical Modeling",
    "section": "",
    "text": "Statistical modeling is the process of building a mathematical representation of the relationships in your data. A model captures the patterns and structure in observed data, allowing you to make predictions, test hypotheses, and understand which variables drive outcomes.\nThe modeling workflow starts with exploratory data analysis — understanding your data before fitting any model. Visualizing distributions, checking for correlations, and identifying potential confounders are essential steps before writing a single line of modeling code."
  },
  {
    "objectID": "modeling.html#linear-regression",
    "href": "modeling.html#linear-regression",
    "title": "Statistical Modeling",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is the starting point for most modeling tasks. It assumes a linear relationship between one or more predictor variables and a continuous outcome. The model finds the line (or hyperplane) that minimizes the sum of squared differences between predicted and observed values.\nimport statsmodels.api as sm\n\nX = sm.add_constant(df[[\"square_feet\", \"bedrooms\", \"age\"]])\nmodel = sm.OLS(df[\"price\"], X).fit()\nprint(model.summary())\nThe regression summary provides coefficients, p-values, and confidence intervals for each predictor. A coefficient tells you the expected change in the outcome for a one-unit increase in that predictor, holding all other variables constant.\nInterpreting regression output requires care. A statistically significant coefficient does not imply causation. Multicollinearity between predictors can inflate standard errors and make individual coefficients unreliable. Always check residual plots to verify that model assumptions hold."
  },
  {
    "objectID": "modeling.html#classification-models",
    "href": "modeling.html#classification-models",
    "title": "Statistical Modeling",
    "section": "Classification Models",
    "text": "Classification Models\nWhen the outcome is categorical rather than continuous, classification models are appropriate. Logistic regression extends linear regression to binary outcomes by modeling the log-odds of the positive class.\nFor more complex classification tasks, tree-based methods like random forests and gradient boosting machines often outperform logistic regression. These models capture nonlinear relationships and interactions between variables without requiring explicit specification.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nscores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\"accuracy\")\nprint(f\"Mean accuracy: {scores.mean():.3f}\")\nModel evaluation for classification goes beyond accuracy. The confusion matrix, precision, recall, and the ROC curve each tell a different part of the story. In imbalanced data settings, accuracy can be misleading — a model that always predicts the majority class achieves high accuracy but is useless."
  },
  {
    "objectID": "modeling.html#model-selection-and-validation",
    "href": "modeling.html#model-selection-and-validation",
    "title": "Statistical Modeling",
    "section": "Model Selection and Validation",
    "text": "Model Selection and Validation\nChoosing between competing models requires a principled approach. Cross-validation estimates how well a model generalizes to unseen data by training and testing on different subsets. The bias-variance tradeoff guides model complexity — simple models may underfit while complex models risk overfitting.\nInformation criteria like AIC and BIC balance model fit against complexity. A lower value indicates a better model, penalizing unnecessary parameters. These criteria are particularly useful for comparing regression models with different sets of predictors."
  },
  {
    "objectID": "modeling.html#regularization",
    "href": "modeling.html#regularization",
    "title": "Statistical Modeling",
    "section": "Regularization",
    "text": "Regularization\nWhen you have many predictors relative to observations, regularization prevents overfitting by adding a penalty term to the loss function. Ridge regression (L2 penalty) shrinks coefficients toward zero, while Lasso regression (L1 penalty) can set coefficients exactly to zero, performing variable selection.\nfrom sklearn.linear_model import LassoCV\n\nlasso = LassoCV(cv=5, random_state=42)\nlasso.fit(X_train, y_train)\nselected = [f for f, c in zip(feature_names, lasso.coef_) if c != 0]\nprint(f\"Selected {len(selected)} of {len(feature_names)} features\")\nElastic net combines both penalties, offering a middle ground that handles correlated predictors better than Lasso alone. The mixing parameter controls the balance between L1 and L2 regularization, and cross-validation selects the optimal penalty strength."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Search Highlight Demo",
    "section": "",
    "text": "This site demonstrates the search highlight fix from quarto-dev/quarto-cli#14047. When you navigate to a page with a ?q=term parameter in the URL, matching terms are highlighted with a yellow background. This is the same mechanism used when you click a search result in the Quarto search box."
  },
  {
    "objectID": "index.html#about-this-demo",
    "href": "index.html#about-this-demo",
    "title": "Search Highlight Demo",
    "section": "",
    "text": "This site demonstrates the search highlight fix from quarto-dev/quarto-cli#14047. When you navigate to a page with a ?q=term parameter in the URL, matching terms are highlighted with a yellow background. This is the same mechanism used when you click a search result in the Quarto search box."
  },
  {
    "objectID": "index.html#how-to-test",
    "href": "index.html#how-to-test",
    "title": "Search Highlight Demo",
    "section": "How to Test",
    "text": "How to Test\n\nDirect URL highlighting (?q=)\nTry these links to see search highlighting in action:\n\nPandas page with “data” highlighted\nVisualization page with “chart” highlighted\nModeling page with “regression” highlighted\n\nThese use the ?q=term URL parameter, which triggers JavaScript-based &lt;mark&gt; highlighting on the destination page. The highlights clear when you scroll enough to change the active section — this is by design.\n\n\nThrough the search box\n\nClick the search icon in the navbar (or press S)\nType a term like “regression”\nClick on one of the search results\n\nThe destination page should scroll to the matching content with JS &lt;mark&gt; highlights (yellow background on matching terms, cleared on scroll).\n\n\nTwo highlight mechanisms\nQuarto search results include two highlight mechanisms in their URLs:\n\nJS &lt;mark&gt; highlights via ?q= parameter — yellow background on matching terms, cleared on scroll (PR #14049)\nBrowser text fragments via #:~:text= in the URL — browser-native highlighting (PR #14003)\n\n\n\n\n\n\n\nNoteText fragments vs JS marks on fuse.js sites\n\n\n\nOn sites using fuse.js search (like this one), the two mechanisms don’t coexist. The highlight() function mutates the DOM by wrapping matches in &lt;mark&gt; elements, and this DOM mutation destroys the browser’s text fragment highlight state. As a result, only the JS marks are visible.\nOn sites using Algolia search (like quarto.org), there is no ?q= parameter and highlight() never runs, so text fragments from PR #14003 are the primary highlight mechanism and work as expected."
  },
  {
    "objectID": "index.html#the-bug",
    "href": "index.html#the-bug",
    "title": "Search Highlight Demo",
    "section": "The Bug",
    "text": "The Bug\nIn previous versions of Quarto, search highlights were cleared almost immediately after page load. Layout-settling events (quarto-hrChanged and quarto-sectionChanged) fired within the first ~24ms and triggered a defined(defined()) call chain that wiped the &lt;mark&gt; elements before users could see them."
  },
  {
    "objectID": "index.html#the-fix",
    "href": "index.html#the-fix",
    "title": "Search Highlight Demo",
    "section": "The Fix",
    "text": "The Fix\nThe fix delays the registration of highlight-clearing event listeners by 1000ms, giving the initial layout events time to settle before the listeners become active. This ensures that search highlights remain visible when navigating from search results.\nSee PR #14049 for the implementation details."
  },
  {
    "objectID": "index.html#content-for-searching",
    "href": "index.html#content-for-searching",
    "title": "Search Highlight Demo",
    "section": "Content for Searching",
    "text": "Content for Searching\nThis site includes several pages with data science content to provide realistic search targets. Each page covers a different topic — data manipulation with pandas, data visualization, and statistical modeling — with enough prose to exercise the search and highlight functionality across multiple pages."
  },
  {
    "objectID": "pandas.html",
    "href": "pandas.html",
    "title": "Data Manipulation with Pandas",
    "section": "",
    "text": "Pandas is the foundational library for data manipulation in Python. A DataFrame is a two-dimensional labeled data structure with columns that can hold different data types. You can think of it as a spreadsheet or SQL table represented in Python.\nWhen you load data into a DataFrame, pandas provides a rich set of methods to inspect, clean, and transform that data. The head() and info() methods give you a quick overview of your data, while describe() produces summary statistics for numeric columns."
  },
  {
    "objectID": "pandas.html#working-with-dataframes",
    "href": "pandas.html#working-with-dataframes",
    "title": "Data Manipulation with Pandas",
    "section": "",
    "text": "Pandas is the foundational library for data manipulation in Python. A DataFrame is a two-dimensional labeled data structure with columns that can hold different data types. You can think of it as a spreadsheet or SQL table represented in Python.\nWhen you load data into a DataFrame, pandas provides a rich set of methods to inspect, clean, and transform that data. The head() and info() methods give you a quick overview of your data, while describe() produces summary statistics for numeric columns."
  },
  {
    "objectID": "pandas.html#selecting-and-filtering-data",
    "href": "pandas.html#selecting-and-filtering-data",
    "title": "Data Manipulation with Pandas",
    "section": "Selecting and Filtering Data",
    "text": "Selecting and Filtering Data\nOne of the most common operations is selecting subsets of your data. You can select columns by name, filter rows based on conditions, or combine both operations. Boolean indexing lets you write expressive queries directly against your DataFrame.\nimport pandas as pd\n\ndf = pd.read_csv(\"sales_data.csv\")\nrecent = df[df[\"date\"] &gt; \"2024-01-01\"]\nhigh_value = recent[recent[\"amount\"] &gt; 1000]\nFiltering data based on multiple conditions is straightforward with the & and | operators. Each condition must be wrapped in parentheses to ensure correct evaluation order."
  },
  {
    "objectID": "pandas.html#grouping-and-aggregation",
    "href": "pandas.html#grouping-and-aggregation",
    "title": "Data Manipulation with Pandas",
    "section": "Grouping and Aggregation",
    "text": "Grouping and Aggregation\nThe groupby() method is central to data analysis with pandas. It splits your data into groups based on one or more columns, applies an aggregation function, and combines the results. This pattern — split, apply, combine — is the backbone of most data summarization tasks.\nsummary = df.groupby(\"region\").agg(\n    total_sales=(\"amount\", \"sum\"),\n    avg_order=(\"amount\", \"mean\"),\n    order_count=(\"amount\", \"count\")\n)\nYou can group by multiple columns to create hierarchical summaries. The resulting data can be pivoted, unstacked, or flattened depending on your reporting needs."
  },
  {
    "objectID": "pandas.html#handling-missing-data",
    "href": "pandas.html#handling-missing-data",
    "title": "Data Manipulation with Pandas",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\nReal-world data almost always contains missing values. Pandas represents missing data as NaN (Not a Number) and provides tools to detect, remove, or fill these gaps. The isna() method identifies missing values, while fillna() and dropna() let you handle them.\nChoosing how to handle missing data depends on your analysis goals. Dropping rows with missing values is simple but can reduce your dataset significantly. Filling with the mean or median preserves row count but can introduce bias. Domain knowledge should guide the decision."
  },
  {
    "objectID": "pandas.html#merging-and-joining-data",
    "href": "pandas.html#merging-and-joining-data",
    "title": "Data Manipulation with Pandas",
    "section": "Merging and Joining Data",
    "text": "Merging and Joining Data\nCombining data from multiple sources is a routine task in data analysis. Pandas provides merge() for SQL-style joins and concat() for stacking DataFrames vertically or horizontally. The merge operation matches rows based on shared key columns, similar to a JOIN in SQL.\nUnderstanding the different join types — inner, left, right, and outer — is essential for combining data correctly. An inner join keeps only rows with matching keys in both DataFrames, while a left join preserves all rows from the left DataFrame regardless of whether a match exists."
  }
]